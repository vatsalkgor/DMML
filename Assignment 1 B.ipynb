{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the jupyter notebook for DM&ML Assignment 1 Part B Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier,plot_tree\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score,auc,accuracy_score,classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  class  b1  b2  b3   b4  b5   b6   b7  b8  b9  ...  pred_minus_obs_H_b9  \\\n",
      "0    d   39  36  57   91  59  101   93  27  60  ...                -2.36   \n",
      "1    h   84  30  57  112  51   98   92  26  62  ...                -2.26   \n",
      "2    s   53  25  49   99  51   93   84  26  58  ...                -1.46   \n",
      "3    s   59  26  49  103  47   92   82  25  56  ...                 2.68   \n",
      "4    d   57  49  66  103  64  106  114  28  59  ...                -2.94   \n",
      "\n",
      "   pred_minus_obs_S_b1  pred_minus_obs_S_b2  pred_minus_obs_S_b3  \\\n",
      "0               -18.41                -1.88                -6.43   \n",
      "1               -16.27                -1.95                -6.25   \n",
      "2               -15.92                -1.79                -4.64   \n",
      "3               -13.77                -2.53                -6.34   \n",
      "4               -21.74                -1.64                -4.62   \n",
      "\n",
      "   pred_minus_obs_S_b4  pred_minus_obs_S_b5  pred_minus_obs_S_b6  \\\n",
      "0               -21.03                -1.60                -6.18   \n",
      "1               -18.79                -1.99                -6.18   \n",
      "2               -17.73                -0.48                -4.69   \n",
      "3               -22.03                -2.34                -6.60   \n",
      "4               -23.74                -0.85                -5.50   \n",
      "\n",
      "   pred_minus_obs_S_b7  pred_minus_obs_S_b8  pred_minus_obs_S_b9  \n",
      "0               -22.50                -5.20                -7.86  \n",
      "1               -23.41                -8.87               -10.83  \n",
      "2               -19.97                -4.10                -7.07  \n",
      "3               -27.10                -7.99               -10.81  \n",
      "4               -22.83                -2.74                -5.84  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "raw_data = pd.read_excel('Forest.xlsx')\n",
    "print(raw_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random_state randomize the samples. for each random state sampling will be different. but for same random state sampling will remain same for each run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = raw_data.iloc[:,1:].values\n",
    "labels = raw_data.iloc[:,0].values\n",
    "pred_train, pred_test, tar_train, tar_test  = train_test_split(attributes,labels,test_size=0.3,random_state=30,stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_DT = DecisionTreeClassifier(criterion=\"entropy\", random_state=999, min_samples_split=150)\n",
    "classifier_DT.fit(pred_train,tar_train)\n",
    "predictions_DT = classifier_DT.predict(pred_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8726114649681529\n",
      "0.8726114649681529\n",
      "0.8726114649681529\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(tar_test, predictions_DT))\n",
    "precision = precision_score(y_true=tar_test, y_pred=predictions_DT, average='micro')\n",
    "print(precision)\n",
    "recall = recall_score(y_true=tar_test, y_pred=predictions_DT, average='micro')\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          d        0.83      0.94      0.88        48\n",
      "          h        0.85      0.85      0.85        26\n",
      "          o        1.00      0.76      0.86        25\n",
      "          s        0.88      0.88      0.88        58\n",
      "\n",
      "    accuracy                           0.87       157\n",
      "   macro avg       0.89      0.86      0.87       157\n",
      "weighted avg       0.88      0.87      0.87       157\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(tar_test,predictions_DT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[45  0  0  3]\n",
      " [ 0 22  0  4]\n",
      " [ 6  0 19  0]\n",
      " [ 3  4  0 51]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(tar_test,predictions_DT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_Classifier = MLPClassifier(max_iter=500)\n",
    "MLP_Classifier.fit(pred_train,np.ravel(tar_train,order='C'))\n",
    "predictions_MLP = MLP_Classifier.predict(pred_test)\n",
    "# print(predictions_MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          d        0.92      0.94      0.93        48\n",
      "          h        0.82      0.88      0.85        26\n",
      "          o        1.00      0.84      0.91        25\n",
      "          s        0.92      0.93      0.92        58\n",
      "\n",
      "    accuracy                           0.91       157\n",
      "   macro avg       0.91      0.90      0.90       157\n",
      "weighted avg       0.91      0.91      0.91       157\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(tar_test,predictions_MLP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "probDT = classifier_DT.predict_proba(pred_test)\n",
    "probMLP = MLP_Classifier.predict_proba(pred_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structure for probabilites array\n",
    "[\n",
    "    [\n",
    "        {d: [DT\tMLP]}\n",
    "        {h: [DT\tMLP]}\n",
    "        {o: [DT\tMLP]}\n",
    "        {s: [DT\tMLP]}\n",
    "    ],\n",
    "    ...\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t DecisionTree \t\t MLP\n",
      "d \t [0.036231884057971016, 0.002944267494963483]\n",
      "h \t [0.050724637681159424, 5.6783123588959015e-05]\n",
      "o \t [0.007246376811594203, 0.0006896971399021482]\n",
      "s \t [0.9057971014492754, 0.9963092522415454]\n"
     ]
    }
   ],
   "source": [
    "probabilities = []\n",
    "for i in range(len(pred_test)):\n",
    "    probabilities.append({'d':[probDT[i][0],probMLP[i][0]],'h':[probDT[i][1],probMLP[i][1]],'o':[probDT[i][2],probMLP[i][2]],'s':[probDT[i][3],probMLP[i][3]]})\n",
    "firstSampleProbability = probabilities[0]\n",
    "print(\"\\t DecisionTree \\t\\t MLP\")\n",
    "for sampleProbability in probabilities:\n",
    "    for probs in sampleProbability:\n",
    "        print(probs,'\\t',sampleProbability[probs])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.3\n",
    "##### Pseudo code:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "function averageProbablePrediction (argument 1){\n",
    "       Convert argument 1 to 2d array\n",
    "       Set average probabilites to empty list \n",
    "       Use different trained classifier to predict probabilities of each class\n",
    "       find average matrix \n",
    "       convert matrix to list\n",
    "       FOR each entry in the list\n",
    "           find index with max value in avg list\n",
    "           find class name for that index\n",
    "           append class name to average probabilities\n",
    "       return average probabilities\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for average aggregate function prediction: 0.8853503184713376\n"
     ]
    }
   ],
   "source": [
    "def averageProbablePrediction(testSet):\n",
    "    global np\n",
    "    testSet = np.atleast_2d(testSet) # if user enters only one sample we convert it to 2D array. \n",
    "    averageProbabilities = []\n",
    "    a,b = classifier_DT.predict_proba(testSet),MLP_Classifier.predict_proba(testSet)\n",
    "    averages = (np.add(a,b)/2).tolist()\n",
    "    for avg in averages:\n",
    "        maxValueIndex = avg.index(max(avg))\n",
    "        averageProbabilities.append(classifier_DT.classes_[maxValueIndex])\n",
    "    return np.array(averageProbabilities)\n",
    "averagePredictions = averageProbablePrediction(pred_test)\n",
    "print(\"Accuracy for average aggregate function prediction:\",accuracy_score(tar_test,averagePredictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[45,  0,  0,  3],\n",
       "       [ 0, 22,  0,  4],\n",
       "       [ 6,  0, 19,  0],\n",
       "       [ 3,  4,  0, 51]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(tar_test,predictions_DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function returns conditional probability for each class for a given set of predictions\"\"\"\n",
    "def condProbs(predictions):\n",
    "    global np\n",
    "    cm = confusion_matrix(tar_test,predictions)\n",
    "    colSum = np.sum(cm,axis=0) # gives sum of columns in a 1D array\n",
    "    correctlyPredicted = cm.diagonal() #gives diagonal elements in 1D array\n",
    "    return np.divide(correctlyPredicted,colSum) # return element by element division of the two array ultimately giving conditional probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditionalProbabilityPrediction(testSet):\n",
    "    global np\n",
    "    conditionalProbabilities = [] # array that will contain all the predicted values at the end \n",
    "    dtCondProbs = condProbs(predictions_DT) #gives conditioal probabilty for each class for predictions of DT classifier\n",
    "    mlpCondProbs = condProbs(predictions_MLP) #gives conditioal probabilty for each class for predictions of MLP classifier\n",
    "    a,b = classifier_DT.predict_proba(testSet),MLP_Classifier.predict_proba(testSet) \n",
    "    for i in range(len(a)):\n",
    "        dt = a[i].tolist()\n",
    "        mlp = b[i].tolist()\n",
    "        p1 = max(a[i]) * dtCondProbs[dt.index(max(a[i]))] # calculate p1 = max of probabilities * conditional probability of getting the class which has max probability for DT\n",
    "        p2 = max(b[i]) * mlpCondProbs[mlp.index(max(b[i]))] # claculate p2 = same as p2 but with probabilities for MLP\n",
    "        if p1 > p2:\n",
    "            conditionalProbabilities.append(MLP_Classifier.classes_[dt.index(max(a[i]))])\n",
    "        else:\n",
    "            conditionalProbabilities.append(MLP_Classifier.classes_[mlp.index(max(b[i]))])\n",
    "    return np.array(conditionalProbabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for conditional prediction: 0.8789808917197452\n"
     ]
    }
   ],
   "source": [
    "conditionalPredictions = conditionalProbabilityPrediction(pred_test)\n",
    "print(\"Accuracy for conditional prediction:\",accuracy_score(tar_test,conditionalPredictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
