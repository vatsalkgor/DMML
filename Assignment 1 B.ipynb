{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the jupyter notebook for DM&ML Assignment 1 Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score,auc,accuracy_score,classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  class  b1  b2  b3   b4  b5   b6   b7  b8  b9  ...  pred_minus_obs_H_b9  \\\n",
      "0    d   39  36  57   91  59  101   93  27  60  ...                -2.36   \n",
      "1    h   84  30  57  112  51   98   92  26  62  ...                -2.26   \n",
      "2    s   53  25  49   99  51   93   84  26  58  ...                -1.46   \n",
      "3    s   59  26  49  103  47   92   82  25  56  ...                 2.68   \n",
      "4    d   57  49  66  103  64  106  114  28  59  ...                -2.94   \n",
      "\n",
      "   pred_minus_obs_S_b1  pred_minus_obs_S_b2  pred_minus_obs_S_b3  \\\n",
      "0               -18.41                -1.88                -6.43   \n",
      "1               -16.27                -1.95                -6.25   \n",
      "2               -15.92                -1.79                -4.64   \n",
      "3               -13.77                -2.53                -6.34   \n",
      "4               -21.74                -1.64                -4.62   \n",
      "\n",
      "   pred_minus_obs_S_b4  pred_minus_obs_S_b5  pred_minus_obs_S_b6  \\\n",
      "0               -21.03                -1.60                -6.18   \n",
      "1               -18.79                -1.99                -6.18   \n",
      "2               -17.73                -0.48                -4.69   \n",
      "3               -22.03                -2.34                -6.60   \n",
      "4               -23.74                -0.85                -5.50   \n",
      "\n",
      "   pred_minus_obs_S_b7  pred_minus_obs_S_b8  pred_minus_obs_S_b9  \n",
      "0               -22.50                -5.20                -7.86  \n",
      "1               -23.41                -8.87               -10.83  \n",
      "2               -19.97                -4.10                -7.07  \n",
      "3               -27.10                -7.99               -10.81  \n",
      "4               -22.83                -2.74                -5.84  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "raw_data = pd.read_excel('Forest.xlsx')\n",
    "print(raw_data.head())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "random_state randomize the samples. for each random state sampling will be different. but for same random state sampling will remain same for each run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = raw_data.iloc[:,1:].values\n",
    "labels = raw_data.iloc[:,0].values\n",
    "pred_train, pred_test, tar_train, tar_test  = train_test_split(attributes,labels,test_size=0.3,random_state=30,stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_DT = DecisionTreeClassifier(criterion=\"entropy\", random_state=999, max_depth=4)\n",
    "classifier_DT.fit(pred_train,tar_train)\n",
    "predictions = classifier_DT.predict(pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8789808917197452\n",
      "0.8789808917197452\n",
      "0.8789808917197452\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(tar_test, predictions))\n",
    "precision = precision_score(y_true=tar_test, y_pred=predictions, average='micro')\n",
    "print(precision)\n",
    "recall = recall_score(y_true=tar_test, y_pred=predictions, average='micro')\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          d        0.85      0.92      0.88        48\n",
      "          h        0.88      0.85      0.86        26\n",
      "          o        1.00      0.72      0.84        25\n",
      "          s        0.87      0.93      0.90        58\n",
      "\n",
      "    accuracy                           0.88       157\n",
      "   macro avg       0.90      0.85      0.87       157\n",
      "weighted avg       0.89      0.88      0.88       157\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(tar_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[44  0  0  4]\n",
      " [ 0 22  0  4]\n",
      " [ 7  0 18  0]\n",
      " [ 1  3  0 54]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(tar_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_Classifier = MLPClassifier(max_iter=500)\n",
    "MLP_Classifier.fit(pred_train,np.ravel(tar_train,order='C'))\n",
    "predictions_MLP = MLP_Classifier.predict(pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          d        0.92      0.96      0.94        48\n",
      "          h        0.76      0.85      0.80        26\n",
      "          o        1.00      0.92      0.96        25\n",
      "          s        0.93      0.88      0.90        58\n",
      "\n",
      "    accuracy                           0.90       157\n",
      "   macro avg       0.90      0.90      0.90       157\n",
      "weighted avg       0.91      0.90      0.91       157\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(tar_test,predictions_MLP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "probDT = classifier_DT.predict_proba(pred_test)\n",
    "probMLP = MLP_Classifier.predict_proba(pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t ['d ' 'h ' 'o ' 's ']\n",
      "Decision Tree\t [0.04032258064516129, 0.008064516129032258, 0.008064516129032258, 0.9435483870967742]\n",
      "MLP Classifier\t [0.0003590394314074509, 0.004468106916179271, 0.000743252402041874, 0.9944296012503714]\n"
     ]
    }
   ],
   "source": [
    "probabilities = []\n",
    "for i in range(len(pred_test)):\n",
    "#     entry=[]\n",
    "#     entry.append(probDT[i].tolist())\n",
    "#     entry.append(probMLP[i].tolist())\n",
    "    probabilities.append([probDT[i].tolist(),probMLP[i].tolist()])\n",
    "firstSampleProbability = probabilities[0]\n",
    "print(\"\\t\\t\",MLP_Classifier.classes_)\n",
    "print(\"Decision Tree\\t\",firstSampleProbability[0])\n",
    "print(\"MLP Classifier\\t\",firstSampleProbability[1])\n",
    "# for probability in probabilities:\n",
    "#     print(\"Decision Tree\\t\",probability[0])\n",
    "#     print(\"MLP Classifier\\t\",probability[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "There needs to be some average aggregate functions "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
